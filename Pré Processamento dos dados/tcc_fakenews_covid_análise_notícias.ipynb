{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcc_fakenews_covid- análise notícias.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Etapa 0)** Importando a biblioteca NLTk - Importar todas!"
      ],
      "metadata": {
        "id": "rToUe9tRBXn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk \n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "VyNTjVKWBbjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install unicodedata\n",
        "import unidecode\n",
        "import unicodedata\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "N7KtlEyJP8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Etapa 1)** Abrindo arquivos do drive\n",
        "\n",
        "Arquivo de notícias **verdadeiras**"
      ],
      "metadata": {
        "id": "RLcqs0eAR_xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_true = open(\"/content/Verdadeiro.txt\").readlines()\n",
        "#print(corpus_true)"
      ],
      "metadata": {
        "id": "OZhmfG2PSBcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arquivo de notícias **falsas**"
      ],
      "metadata": {
        "id": "Q2q9lf4sTnKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_false = open(\"/content/falso.txt\").read()\n",
        "#print(corpus_false)"
      ],
      "metadata": {
        "id": "x8c7z5QvTp7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Etapa 2 - Tokenizar e calcular frequências**\n",
        "\n",
        "**2.1)** Notícias **VERDADEIRAS:**\n",
        "\n",
        "1) Sem pontuações\n",
        "\n",
        "2) Sem números\n",
        "\n",
        "3) Sem stop words\n",
        "\n",
        "4) Em minúsculas"
      ],
      "metadata": {
        "id": "JWqAu37CI7hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Retirando acentos Noticias Verdadeiras- Codificacao \n",
        "\n",
        "corpus_true_unicode = unidecode.unidecode(corpus_true)\n",
        "print(corpus_true_unicode) "
      ],
      "metadata": {
        "id": "ORsNDFKYP_tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "#Utilizando as stopwords do nltk\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese') \n",
        "\n",
        "#Método do nltk para considerar somente letras e retirar pontuação\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*') \n",
        "#Método do nltk para tokenizar palavras\n",
        "tokens_true = tokenizer.tokenize(corpus_true_unicode)  \n",
        "\n",
        "tokens_true_sem_stopwords = []\n",
        "for token in tokens_true:\n",
        "  # Remove as stopwords\n",
        "  if token.lower() not in stopwords: \n",
        "    # Coloca os tokens em minúsculas, o que é imporante para a frequência\n",
        "    tokens_true_sem_stopwords.append(token.lower()) \n",
        "\n",
        "#print(tokens_true_sem_stopwords)\n",
        "frequencia_true = nltk.FreqDist(tokens_true_sem_stopwords)\n",
        "frequencia_true.most_common(15)"
      ],
      "metadata": {
        "id": "RN8TEH6lJAqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus_true)"
      ],
      "metadata": {
        "id": "ofAyrPATioV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Números calculados pela contagem total de palavras presentes no texto - Verdadeiro\n",
        "n_words = len(tokens_true_sem_stopwords)\n",
        "frequencia_true_relativa = list(map(lambda x: (x[0], x[1]/n_words), frequencia_true.most_common()))\n",
        "frequencia_true_relativa\n",
        "#sum([x[1] for x in frequencia_true_relativa])\n"
      ],
      "metadata": {
        "id": "bBXKEUHJi5Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2)** Notícias **FALSAS**:\n",
        "\n",
        "1) Sem pontuações\n",
        "\n",
        "2) Sem números\n",
        "\n",
        "3) Sem stop words\n",
        "\n",
        "4) Em minúsculas"
      ],
      "metadata": {
        "id": "s1Hg47n1Q5lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Retirando acentos Noticias Verdadeiras- Codificacao -- (Ainda em teste)\n",
        "\n",
        "corpus_false_unicode = unidecode.unidecode(corpus_false)\n",
        "print(corpus_false_unicode) "
      ],
      "metadata": {
        "id": "lHOfOhxCWcwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese') #Utilizando as stopwords do nltk\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*') #Método do nltk para considerar somente letras e retirar pontuação\n",
        "tokens_false = tokenizer.tokenize(corpus_false_unicode) #Método do nltk para tokenizar palavras\n",
        "\n",
        "tokens_false_sem_stopwords = []\n",
        "for token in tokens_false:\n",
        "  if token.lower() not in stopwords: # Remove as stopwords\n",
        "    tokens_false_sem_stopwords.append(token.lower()) # Coloca os tokens em minúsculas, o que é imporante para a frequência\n",
        "\n",
        "#print(tokens_false_sem_stopwords)\n",
        "frequencia_false = nltk.FreqDist(tokens_false_sem_stopwords)\n",
        "frequencia_false.most_common(15)"
      ],
      "metadata": {
        "id": "L0ZH8fdEQ-4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus_false)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpEp1_4vXSh",
        "outputId": "c6523620-92ef-4f49-8d14-8a3e1a261876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58671"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Números calculados pela contagem total de palavras presentes no texto - Falso\n",
        "\n",
        "n_words = len(tokens_false_sem_stopwords)\n",
        "frequencia_false_relativa = list(map(lambda x: (x[0], x[1]/n_words), frequencia_false.most_common()))\n",
        "frequencia_false_relativa\n",
        "#sum([x[1] for x in frequencia_false_relativa])"
      ],
      "metadata": {
        "id": "8ZXJNMZwtx_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Etapa 3 - N-gramas**\n",
        "\n",
        "**3.1)** N-gramas das notícias **VERDADEIRAS**\n"
      ],
      "metadata": {
        "id": "wYJdgCw4RLll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Utilizando o método de extração de n-gramas para o corpus COM stopwords\n",
        "list(ngrams(tokens_true,3))\n",
        "#list(ngrams(tokens_true,4))\n",
        "#list(ngrams(tokens_true,5))\n",
        "#list(ngrams(tokens_true,6))"
      ],
      "metadata": {
        "id": "z_woahK5RRNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.1)** N-gramas das notícias **VERDADEIRAS (sem stopwords)**"
      ],
      "metadata": {
        "id": "z00EWQBHFOin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Utilizando o método de extração de n-gramas para o corpus SEM stopwords\n",
        "list(ngrams(tokens_true_sem_stopwords,3))\n",
        "#list(ngrams(tokens_true_sem_stopwords,4))\n",
        "#list(ngrams(tokens_true_sem_stopwords,5))\n",
        "#list(ngrams(tokens_true_sem_stopwords,6))"
      ],
      "metadata": {
        "id": "3EyuTACBFVJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2)** N-gramas das notícias **FALSAS**\n"
      ],
      "metadata": {
        "id": "0_K9sqE6h3OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Utilizando o método de extração de n-gramas para o corpus COM stopwords\n",
        "#list(ngrams(tokens_false,3))\n",
        "#list(ngrams(tokens_false,4))\n",
        "#list(ngrams(tokens_false,5))\n",
        "list(ngrams(tokens_false,6))"
      ],
      "metadata": {
        "id": "IceKokOHh6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.1)** N-gramas das notícias **FALSAS (sem stopwords)**"
      ],
      "metadata": {
        "id": "K5Fn0YIJFkTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Utilizando o método de extração de n-gramas para o corpus SEM stopwords\n",
        "list(ngrams(tokens_false_sem_stopwords,3))\n",
        "#list(ngrams(tokens_false_sem_stopwords,4))\n",
        "#list(ngrams(tokens_false_sem_stopwords,5))\n",
        "#list(ngrams(tokens_false_sem_stopwords,6))"
      ],
      "metadata": {
        "id": "DZTKpUxaFpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Etapa 4 - Etiquetadores**\n",
        "\n",
        "**4.1)** Notícias **VERDADEIRAS**"
      ],
      "metadata": {
        "id": "tssxGiUeHNoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "\n",
        "# Corpus verdadeiro\n",
        "tokens_true = nltk.word_tokenize(corpus_true_unicode)\n",
        "sentencas_treino_true = mac_morpho.tagged_sents()\n",
        "# Função etiquetador\n",
        "etiquetador_true = UnigramTagger(sentencas_treino_true)\n",
        "etiquetado_true = etiquetador_true.tag(tokens_true)\n",
        "\n",
        "etiquetado_true\n",
        "\n"
      ],
      "metadata": {
        "id": "8OiuDzthHV0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2** - Notícias **FALSAS**"
      ],
      "metadata": {
        "id": "yPkkhTwKLLad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "\n",
        "# Corpus falso\n",
        "tokens_false = nltk.word_tokenize(corpus_false_unicode)\n",
        "sentencas_treino_false = mac_morpho.tagged_sents()\n",
        "# Função etiquetador\n",
        "etiquetador_false = UnigramTagger(sentencas_treino_false)\n",
        "etiquetado_false = etiquetador_false.tag(tokens_false)\n",
        "\n",
        "etiquetado_false"
      ],
      "metadata": {
        "id": "B1D0EcI9K0lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vetorização**\n"
      ],
      "metadata": {
        "id": "qHPskKeeHSsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vetorizando Corpus verdadeiro\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(corpus_true)\n",
        "Vetor = count_vect.get_feature_names_out()\n",
        "\n",
        "#Imprimindo o vetor\n",
        "print(Vetor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMw1u6ediHRG",
        "outputId": "d6dfd107-4475-4186-e327-77cbd13a200c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['000' '03' '060' ... 'único' 'úteis' 'útero']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analisando Primeiros Valores núméricos e identificado que são números nas noticias \"5.000 casos\"\n",
        "print(Vetor[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYbP_8ynjUB8",
        "outputId": "236c7b00-c315-4226-bda9-a4dd88f8a956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificado que na posição 100 do array possui o número 000 que remete ao \"5.000\"\n",
        "VetorVerdadeiro = X_train_counts.toarray()\n",
        "print(vetorVerdadeiro[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-igaMcKnxuz",
        "outputId": "fbc98c75-e821-411b-c518-74f4c26597aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tamanho vetor Corpus Verdadeiro\n",
        "tamanho = vetorVerdadeiro.size\n",
        "print(tamanho)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZVMcN4EeuU1",
        "outputId": "61a9e994-725f-4b39-8926-b4f4c961a8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "214704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizando com \n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(5, 5))\n",
        "X2 = vectorizer2.fit_transform(corpus_true)\n",
        "vectorizer2.get_feature_names_out()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYyrFWaOnLRk",
        "outputId": "207e0b9a-98c4-4616-89c8-39c7c73d25b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['000 testes em apenas 24', '03 início de testes de',\n",
              "       '060 mortes se somarmos 2009', ...,\n",
              "       'único para quarentena separando diariamente',\n",
              "       'úteis para contabilizar as mortes',\n",
              "       'útero criança nasceu junto de'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilKiEbPOoUsp",
        "outputId": "c3efba69-c72d-46ea-b356-4a82736e39d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}
